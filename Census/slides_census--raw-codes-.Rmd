---
title: "What identifies high-income people?"
subtitle: "An analysis of Census data"
author: "Langyi Tian"
date: "Jan 2020"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "custom_Xaringan.css"]
    lib_dir: libs
    keep_md: true
    self_contained: false
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
editor_options: 
  chunk_output_type: inline
---
exclude: true

class: left, top
background-image: url(images/roadmap.png)
background-size: 100%
background-position: 50% 280%


## Setup
```{r setup, include=FALSE}
#Set up default knitr chunk options
library("knitr")
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.height = 6,
  fig.width = 16,
  fig.align = "center",
  cache = TRUE,
  cache.lazy = FALSE
)
#knitr::opts_knit$set(root.dir = "C:/Users/Tianl/Documents/GitHub/Bank-Marketing-Machine-Learning/Census")
options(htmltools.dir.version = FALSE)
```  

```{r theme-map, include=FALSE}
#set up theme map
theme_simplemap <- function(base_size = 9,
                            base_family = "") {
  theme_bw(base_size = base_size, base_family = base_family) %+replace%
    theme(
      axis.line = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      axis.title = element_blank(),
      panel.background = element_blank(),
      panel.border = element_blank(),
      panel.grid = element_blank(),
      panel.spacing = unit(0, "lines"),
      plot.background = element_blank(),
      legend.position = "none"
    )
}
```

```{r load packages}
packages <- c(
  "tidyverse",
  "knitr",
  "ggplot2",
  "gridExtra",
  "FactoMineR",
  "factoextra",
  "RColorBrewer",
  "countrycode",
  "qwraps2",
  "sjstats",
  "finalfit",
  "fpc",
  "tidymodels",
  "parsnip"
)
packages <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x)
      library(x, character.only = TRUE)
    }
  }
)
select<-dplyr::select
```

## Data preparation and EDA
Before any modeling, the first step is to get an overview of the information and organization of variables in the dataset, based on which we perform basic data cleaning.
We combine the training and test set during this stage and mark the train/test category in case we need to split them later on. 

```{r import data and print summary}
#import raw data
train <- read.csv("adult.data.csv",
                  header = FALSE,
                  stringsAsFactors = FALSE)
test <- read.csv(
  "adult.test.csv",
  header = FALSE,
  skip = 1,
  stringsAsFactors = FALSE
)
#combine train and test together
dta <- rbind(cbind(set = rep("train", nrow(train)),
                   train),
             cbind(set = rep("test", nrow(test)),
                   test))
#assign variable names
names(dta)[-1] <- c(
  "age",
  "workingclass",
  "fnlwgt",
  "education",
  "education.num",
  "martial.status",
  "occupation",
  "relationship",
  "race",
  "sex",
  "capital.gain",
  "capital.loss",
  "hours.per.week",
  "native.country",
  "income"
)
str(dta)
```

From the structure of data, the majority of the features are categorical.
We also notice a redundant white space before strings. We'll take care of that.
From the documentation, the missing values are coded as '?'. We transform it to the standard 'NA' notation in R.
After the basic manipulations, we print a summary to understand statistical distribution of variables.

```{r print summary}
#the weighting should have nothing to do with prediction
dta <- dta %>% select(-fnlwgt)

#trim the white space
dta <- dta %>% mutate_if(is.character, str_trim)

#recode ? and unknown to NA
dta <- na_if(dta, "?")

#print a summary
summary_table(dta)
```

### Recoding and transforming features
The purpose of the analysis is not pure prediction, but to find the relationship between variables and extract insights for the general audience. 
Therefore, the variables need to be engineered so that the categories should be simple enough to be understood by a normal human reader. 

Several variables (e.g. nation of origin) contain very specific categories with extremely small portions. Such level of details cannot be comprehended by general audience and potentially can cause overfitting. Therefore, some categories are binned to avoid these cases.

Country names have too many values and need to be binned.
I recode countries other than the US, Mexico and Phillipines to the corresponding continent. The chi-square wasn't lowered much. 
```{r recode nation of origin}
dta$native.country[!dta$native.country %in% c("United-States")] <-
  countrycode(dta$native.country[!dta$native.country %in% c("United-States")],
              "country.name",
              "continent",
              nomatch = "Not matched")
```

The capital gain/loss might be combined into capital profit 
```{r recode capital gain}
dta <-
  dta %>% mutate(profit = capital.gain - capital.loss) %>% select(-capital.gain,-capital.loss)
```

The income is coded incorrectly. 
```{r recode income}
#recode income
dta$income[dta$income == "<=50K."] <- "<=50K"
dta$income[dta$income == ">50K."] <- ">50K"
dta$income[dta$income == "<=50K"] <- "Income <= 50K"
dta$income[dta$income == ">50K"] <- "Income > 50K"
```

Of course, we try to minimize the variance loss during binning. Therefore, we produce contigency tables with chi-square to decide which categories to merge together. 
```{r print contigency tables}
for (i in names(dta %>% select(-income))) {
  if (class(dta[[i]]) == "character") {
    i %>% print()
    cramer(as.formula(paste("income~", i, sep = "")), dta) %>% print()
    table(dta[[i]], dta$income) %>% print()
  }
}
```

```{r binning working class martial and race}
#recode workingclass
dta$workingclass[dta$workingclass %in% c("Federal-gov", "Local-gov", "State-gov")] <-
  "Government"
dta$workingclass[dta$workingclass %in% c("Self-emp-inc", "Self-emp-not-inc")] <-
  "Self-employed"
dta$workingclass[dta$workingclass %in% c("Never-worked", "Without-pay")] <-
  "No work/pay"
#recode martial.status
dta$martial.status[dta$martial.status %in% c("Married-AF-spouse", "Married-civ-spouse")] <-
  "Married"
dta$martial.status[dta$martial.status %in% c("Divorced", "Married-spouse-absent", "Separated", "Widowed")] <-
  "post-marriage"
#Race
dta$race[dta$race %in% c("Amer-Indian-Eskimo", "Asian-Pac-Islander")] <-
  "Other"
#Too many categories in occupation and no apparent good way of recoding, leave it as it is
```


### Missing value imputation
Next, we'd deal with the missing values in the data.
```{r print missing value summary}
ff_glimpse(dta)
```
Two factors contain missing values: occupation and working class. We'd like to see whether the missing can be by random or not, so that we can decide about ways to impute them.

```{r observe pattern of missingness}
dta %>% select(workingclass, occupation, native.country) %>% missing_pattern()
```

The missingness in workingclass and occupation highly correlates with each other. We should guess whether it's MAR or MNAR

```{r missing pattern of working class}
dta %>% missing_compare("workingclass",
                        c("age", "education.num", "profit"))
```

The missingness in working-related information seems not by random. Those missing the information tend to be older, receive less education, be less active in investment activities. Therefore, statistical imputing would introduce bias and we simply recode the missing value with "missing" label in order not to lose the "non-randomness" in the missing values.

```{r impute working class and occupation}
dta$workingclass[is.na(dta$workingclass) == TRUE] <- "missing"
dta$occupation[is.na(dta$occupation) == TRUE] <- "missing"
```


For any modeling, we need to transform character to factor.
```{r character to factor}
#recode character to factor
for (i in names(dta)) {
  if (class(dta[[i]]) == "character") {
    dta[[i]] <- dta[[i]] %>% as.factor()
  }
}
```

## EDA
What attributes are associated with higher income? 
The variables in this data are mostly categorical. The way to understand the relationship between variables are thus different from the continous variable case. 

##Visualize relationship between variables
```{r making plots}
#construct summary statistics by education, profit, martial status etc. to set up ggplot data infrastructure
education.plot <- ggplot(
  data = dta %>% group_by(education.group, income) %>% summarise(percent =
                                                                        n() * 100 / nrow(dta)) %>% mutate(group = str_c(education.group, " and income", income)),
  aes(x = education.group,
      y = percent,
      fill = income)
)

sex.plot <- ggplot(
  data = dta %>% group_by(sex, income) %>% summarise(percent =
                                                                     n() * 100 / nrow(dta)) %>% mutate(group = str_c(sex, " and income", income)),
  aes(x = sex,
      y = percent,
      fill = income)
)

martial.status.plot <- ggplot(
  data = dta %>% group_by(martial.status, income) %>% summarise(percent =
                                                                       n() * 100 / nrow(dta)) %>% mutate(group = str_c(martial.status, " and income", income)),
  aes(x = martial.status,
      y = percent,
      fill = income)
)

race.plot <- ggplot(
  data = dta %>% group_by(race, income) %>% summarise(percent =
                                                             n() * 100 / nrow(dta)) %>% mutate(group = str_c(race, " and income", income)),
  aes(x = race,
      y = percent,
      fill = income)
)

#my function to make the plots and make them prettier
formatting <- function(plot, legend = FALSE) {
  plot <- plot +
    geom_bar(stat = "identity") +
    scale_fill_brewer(palette = "Paired") +
    xlab("") +
    ylab("") +
    labs(fill = "Income")  +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 30, hjust = 1))
  if (legend == FALSE) {
    plot <- plot +
      theme(legend.position = "none")
  }
  return(plot)
}

#combine the 4 plots together in 1 page
grid.arrange(
  race.plot %>% formatting(),
  education.plot %>% formatting(),
  martial.status.plot %>% formatting(),
  sex.plot %>% formatting(legend = TRUE),
  left = "Percentage"
)
```

##MCA
Unlike dataset where we can produce a correlation matrix, it's not easy to understand the relationship between categorical and numeric features in mixed data. Since the majority of features in this data are categorical, we create from the few numeric features categorical variables and visualize the relationship between them with MCA, a principal component technique on categorical variables. This method might lose some information when we recode numeric features to categorical variable, but is very beneficial for two purposes for us:
1. Visualizing the relationships of variables in a multi-variate setting.
2. Extract principal components from mixed data

```{r data summary by income group}
summary.stats <-
  cbind(summary_table(dta %>% filter(income == "Income <= 50K")),
        summary_table(dta %>% filter(income == "Income > 50K")))
summary.stats
```

```{r recode numeric to factor for famd}
dta$education.group <- dta$education.num %>% cut(
  c(-Inf, 5, 9, 12, 13, Inf),
  labels = c(
    "Middle school",
    "High school",
    "Associates",
    "Undergrad education",
    "Graduate education"
  )
)
dta$age.group <- dta$age %>% cut(
  c(-Inf, 25, 40, 55, Inf),
  labels = c("<25 yrs old",
             "25-40 yrs old",
             "40-55 yrs old",
             ">55 yrs old")
)
dta$hours.group <- dta$hours.per.week %>% cut(c(-Inf, 39, 40, Inf),
                                              labels = c("Work undertime",
                                                         "Work regular hours",
                                                         "Work overtime"),
)
dta$profit.group <- dta$profit %>% cut(
  c(-Inf, -1, 0, Inf),
  labels = c(
    "Has capital investment",
    "No investment",
    "Has capital investment"
  )
)
```

Because occupation variable contains many categories and there's not a very efficient way to recode them, we didn't include it in the analysis. All other variables are included. 


Here, the MCA is used to visualize categorical groups that have higher likelihood to overlap with higher income groups
```{r construct and visualize MCA}
#select the necessary features into MCA.
#All variables but occupation are included. Too many categories in occupation will make the plot unreadable.
dta_famd <-
  dta %>%  filter(!race %in% c("Other"),!native.country %in% c("Not matched")) %>% select(#get ride of a few outliers 
                      income,
                      sex,
                      education.group,
                      race,
                      martial.status,
                      native.country,
                      age.group,
                      hours.group,
                      profit.group,
                      occupation,
                    )%>%mutate(occupation=occupation%>%str_c(" "))
#Construct the MCA
res.famd <- FAMD (dta_famd,
                  ncp = 2,#This MCA is not for clustering purpose, so we just use 2 components and visualize them on a plot
                  graph = FALSE)

#build the visualization of 2 principal components
quali.var <- get_famd_var(res.famd, "quali.var")
mca.plot<-fviz_famd_var(
  res.famd,
  "quali.var",
  repel = TRUE,
  col.var = "black"
) + 
 theme_void()+
  xlab("") +
  ylab("") +
  labs(title = " ",
       subtitle = "Groups on the left tend to have higher income (on x-axis)",
       caption = "From Multiple Correspondence Analysis (MCA) result")

mca.plot
```


#Modeling
Not only do we want to understand the relationship between income category and other variables, we'd also want to understand how much is the contribution to higher income from each factor. A regression model allows to observe the impact controlling other factors, therefore it's the most appropriate choice given the objective of "What are the key factors contributing to high vs. low income?"

Because of a lot of categorical variables in the data, overfitting can be easy. Therefore, although the purpose is prediction, we use cross-validation to conduct the analysis.

For this part, I use tidymodels, an R package that is similar to scikit-learn library to build and train the model

```{r train test split}
dta_modeling <- dta %>% select(
  set,
  income,
  age,
  workingclass,
  education.num,
  martial.status,
  occupation,
  relationship,
  race,
  sex,
  hours.per.week,
  native.country,
  profit
)
train <- dta_modeling %>% filter(set == "train") %>% select(-set)
test <- dta_modeling %>% filter(set == "test") %>% select(-set)
```

For the feature preprocessing, because the objective of the task is not going for the highes accuracy but have some interpretability, I didn't one-hot-encode, over/undersampling, scale or normalize the features. These steps are useful for better accuracy but make it hard to interpret the coefficients. 
```{r set up formula and preprocessing with tidymodels}
#build formula
formula.1<-formula(income ~ .)

#modeling framework
recipe_simple <- function(dataset) {
  recipe(formula.1, 
         data = dataset) %>%
    step_string2factor(all_nominal(), -all_outcomes()) %>%
    prep(data = dataset)
}

#only use train set for recipe to prevent information leakage between train and test
recipe_prepped <- recipe_simple(dataset = train)

#conduct the preprocessing
train_baked <- bake(recipe_prepped, new_data = train)
test_baked  <- bake(recipe_prepped, new_data = test)
```

Because of the binary nature of outcome variable, we use logistic regression. 
Since we already engineered the features and want to observe the impact from factors, I didn't penalize the regression.

```{r train the glm model}
logistic_glm <-
  logistic_reg(mode = "classification") %>%
  set_engine("glm") %>%
  fit(formula.1, data = train_baked)
```

```{r inspect errors}
#make predictions on test set
predictions_glm <- logistic_glm %>%
  predict(new_data = test_baked) %>%
  bind_cols(test_baked %>% select(income))

#inspect accuracy
accuracy <- predictions_glm %>%
  metrics(income, .pred_class) %>%
  select(-.estimator) %>%
  filter(.metric == "accuracy")

#plot accuracy
prediction.plot<-predictions_glm %>%
  conf_mat(income, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(fill="cyan4",
            show.legend = FALSE) +
  geom_text(aes(label = n),
            colour = "black",
            alpha = 1,
            size = 8) +
  labs(
    title = str_c(
      "Overall prediction accuracy: ",
      100 * accuracy$.estimate %>% round(4),
      "%"
    ),
    subtitle = "Predicting high income group is more challenging",
    caption = "Result from logistic regression model"
  ) +
  theme_classic()
prediction.plot
```


We output the coefficients from regression and visualize their magnitude

```{r inspect the coefficients}
#summary coefficients
summary_logit<-logistic_glm$fit%>%summary()
coefficients_table<-summary_logit$coefficients%>%as.data.frame()
coefficients_table<-bind_cols(var=rownames(coefficients_table),
                          coef=coefficients_table$Estimate*100,
                          p=coefficients_table$`Pr(>|z|)`
                          )%>%as.data.frame()%>%mutate(coef=coef%>%round(2))
coefficients_table<-coefficients_table[order(coefficients_table$coef,decreasing = TRUE),]

#plot first 15 coefficients with p<0.01 significance
coef.plot <- ggplot(
  data = coefficients_table %>% head(15) %>% filter(var != "occupationmissing",
                                                    p<0.01),
  aes(x = var %>% reorder(coef),
      y = coef)
) +
  geom_bar(stat = "identity",
           fill = "darkseagreen3") +
  xlab("") +
  ylab("") +
  labs(title = "Some factors strongly increase individual's likelihood to have high income",
       fill = "Income",
       caption = "Coefficients with p<0.01 from logsitic regression model")  +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

coef.plot
```

##With k-medoid prediction
Here, we use K-medoid algorithm instead of traditional K-means, because we'd like to extract a sample profile from consumer clusters.
The centers of cluster in K-medoid is actual data points, thus the output is more "real" and can serve better as examples. 
K-medoid is more computationally expensive, so I only sample 10% of the data in this assignment. 

Before inputting, I use principal component method to reduce dimensionality
```{r principal component analysis with mixed data on sampled test set}
set.seed(1)
#select and sample the test set 
dta_famd_1 <-
  dta %>%filter(set=="test") %>%select(
    income,
    sex,
    education.num,
    race,
    martial.status,
    relationship,
    native.country,
    age,
    hours.per.week,
    profit,
    workingclass,
    occupation
  )%>%sample_frac(0.1)
#do the analysis
res.famd_1 <- FAMD (dta_famd_1,
                    ncp = 5,
                    sup.var = 1,
                    ind.sup = NULL,
                    graph = FALSE)
km_input<-res.famd_1$ind$coord%>%as.data.frame()
```

To select the best k, I didn't follow the typical method to observe reduced variance.
Alternatively, my goal is to ensure the cluster is stable enough to be a consumer segment, and on this basis, have more segments as possible. 
Therefore, I bootstrap the K-medoid and observe the stability of clusters. 

```{r My own function that bootstraps K-medoid using PAM algorithm from package fpc}
cluster_test <- function(num, nboot) {
  set.seed(1)
  class_pamk <- fpc::clusterboot(
    km_input,
    B = nboot,
    distances = FALSE,
    bootmethod = "boot",
    multipleboot = TRUE,
    dissolution = 0.5,
    recover = 0.7,
    count = FALSE,
    showplots = FALSE,
    clustermethod = fpc::pamkCBI,
    k = num,
    criterion = "asw",
    usepam = TRUE
  )
  class_pamk %>% print()
  return(class_pamk$result$result)
}
```

```{r check stability when k is 3, eval=FALSE}
class_pamk_3 <- cluster_test(3, 50)
```

```{r check stability when k is 4, eval=FALSE}
class_pamk_4<-cluster_test(4,50)
```

```{r check stability when k is 5, eval=FALSE}
class_pamk_5<-cluster_test(5,50)
```

```{r check stability when k is 6, eval=FALSE}
class_pamk_6<-cluster_test(6,50)
```


As a result,k=7 is the optimal solution with stable clusters and maximized K. This formulation might not give maximally rediced variance but can serve as interesting insights to consumer segments.

```{r check stability when k is 7, eval=FALSE}
class_pamk_7<-cluster_test(7,50)
pamk_mod <- class_pamk_7
save(pamk_mod,file="pamk_mod.RData")
```

```{r check stability when k is 8, eval=FALSE}
class_pamk_8<-cluster_test(8,50)
```

```{r check stability when k is 9, eval=FALSE}
class_pamk_9<-cluster_test(9,50)
```

```{r produce the profiles of clusters}
load("pamk_mod.RData")
cluster_size <- pamk_mod$pamobject$clusinfo[, 1]
medoid <- cbind(id = seq(1, nrow(dta_famd_1), 1),
                dta_famd_1) %>% filter(id %in% pamk_mod$pamobject$id.med) %>%
  cbind(size = (cluster_size / nrow(dta_famd_1))) 
predictions_medoid_glm <- cbind(logistic_glm %>%
                                  predict(new_data = medoid, type = "prob"),
                                medoid %>% select(-id))
predictions_medoid_glm<-predictions_medoid_glm[order(predictions_medoid_glm$`.pred_Income > 50K`,decreasing = TRUE),]
predictions_medoid_glm%>%data.table::data.table()
```


---
###Executive Summary

We want to target our marketing/service to higher income people. 

Taking the data, what do we know about them? what can we do to identify them? 

--------------
Summary of findings:

 - Exploratory analysis at group level
 
Characteristics such as white skin, highly educated, prestigous occupation (manager, professional, etc.) and being married are identified to be linked with higher income group. 

 - Individual-level modeling 
 
Male and nation of origin also emerge as foundamental determinants contributing to higher income

 - Identify them in the future 
 
To address future need to identify high-income people, we tested a machine learning algorithm and find it more challenging to predict the higher-income than the lower-income. 

 - Know more about the targeted segment 
 
To understand specific consumer tribes, we mined 7 tribes with differnt likelihood to have high income.

---
### Exploratory analysis: specific groups to target
```{r}
grid.arrange(
  sex.plot %>% formatting() +
    theme(text = element_text(size = 20)),
  education.plot %>% formatting() +
    theme(text = element_text(size = 20)),
  martial.status.plot %>% formatting() +
    theme(text = element_text(size = 20)),
  race.plot %>% formatting(legend = TRUE) +
    theme(text = element_text(size = 20)),
  left = "Percentage"
)
```

 - There are more high-income people among male, the educated, married families, and white people. 
 - At a rough level, targeting at these groups will be more effecitve than reaching everyone
 - However, only observing by one dimension is too rough. e.g., how to compare the advantage of white people and college graduates?
 - The next page gives an overview from multiple dimensions. 

---
### Exploratory analysis: a more comprehensive overview
```{r}
mca.plot+
  theme(text = element_text(size = 20))+
  theme(
  panel.background = element_rect(fill = "lightyellow1"))
```
 - A number of groups consist of married families, college graduates, senior people (40-55 yrs old), and people with good occupations (managers, professionals). People there are likely to have high income--much more likely than male or white people as a whole.
 - But some people are both male and managers. Is it being male or being a manager (or both) that makes his money? How do these two factors compare?
 - If we'd like to understand these, we need an individual-level analysis that controls these attributes.
 
---
### Individual-level modeling: identify influential factors

```{r}
coef.plot+
  theme(text = element_text(size = 20))
```

 - Consistent with findings at the group level, being in marriage, having a good occupation and education will create more chance to have high income
 - Being a male and originating from Asia or Europe actually help a lot more than what seems at group level. Probably, these origin help people get better education, occupation, etc. 

---
### Identify them in the future: machine learning prediction
 - What if we want to identify high income people from our future data without knowing their income? 
 - Machine learning will help us to predict. This is how well we can do at a preliminary stage.

```{r}
prediction.plot+
  theme(text = element_text(size = 20))
```

 - Looks like it's more challenging to find people with higher income than lower income, but surely they are of higher consumer value. Keep improving! 


 
---
### Know more about them 
 - Now we know by what attributes/group membership can we find high income people, and how well we can find them.

 - Finally, what if we want to design a detailed marketing message to some specific consumer segments/tribes? 
 
 - Let' say, we'd like to not only find those with higher income but also know other group-level characteristics such as demographics and occupation, so that we can tailor our strategy? 
 
So, we mined 7 segments in the population that help us understand who we can target and what are they like.

---
### Know more about them: 7 consumer segments

```{r, fig.height = 8}
predictions_medoid_glm %>% mutate(
  `% Probabiliy of >50k income` = `.pred_Income > 50K` %>% round(2) * 100,
  `Segment % in sample` = size %>% round(2) * 100
) %>% select(
  `Sex` =
    sex,
  `Age` =
    age,
  `Marriage` =
    martial.status,
  `Race` =
    race,
  `Education level` =
    education.num,
  `Job` =
    occupation,
  `% Probabiliy of >50k income`,
  `Segment % in sample`
) %>% DT::datatable(rownames=FALSE,
                    
                    caption = "Consumer segment profiles: Cluster medoids with predicted probability of high income")
```

---
###Going forward: applications
 - Identification of high-income individual with machine learning
 - Design targeted marketing messages to specific segments
 - ...
 
 
 THANK YOU!

