---
title: "Predicting academic job salary"
author: "Langyi Tian"
date: "March 2020"
output:
  xaringan::moon_reader:
    css: ["mtheme_max.css", "fonts_mtheme_max.css"]    
    lib_dir: libs
    keep_md: true
    self_contained: false
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
editor_options: 
  chunk_output_type: inline
---
exclude: true

class: left, top
background-image: url(images/roadmap.png)
background-size: 100%
background-position: 50% 280%


```{r setup, include=FALSE}
rm(list = ls())#clear environment
wd <- "~/GitHub/Pet-projects/vote-analysis/"#set working directory
#Set up default knitr chunk options
library("knitr")
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.height = 6,
  fig.width = 15,
  fig.align = "center",
  cache = TRUE,
  cache.lazy = FALSE
)
options(htmltools.dir.version = FALSE)
```  

```{r theme-map, include=FALSE}
theme_simplemap <- function(base_size = 9,
                            base_family = "") {
  theme_bw(base_size = base_size, base_family = base_family) %+replace%
    theme(
      axis.line = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      axis.title = element_blank(),
      panel.background = element_blank(),
      panel.border = element_blank(),
      panel.grid = element_blank(),
      panel.spacing = unit(0, "lines"),
      plot.background = element_blank(),
      legend.position = "none"
    )
}
```

```{r load packages}
packages <- c(
  "tidyverse",
  "tidymodels",
  "tune",
  "vip",
  "ggplot2",
  "finalfit"
)
packages <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x)
      library(x, character.only = TRUE)
    }
  }
)
select<-dplyr::select
```

```{r}
#import raw data
dta <- read.csv("Salaries.csv",
                  stringsAsFactors = FALSE)
```

```{r}
#print a summary
summary(dta)
salary_plot <- ggplot(dta %>% mutate(salary = salary),
                      aes(x = salary)) +
  geom_histogram(fill = "skyblue2") +
  xlab("") +
  ylab("") +
  theme_classic() +
  theme(text = element_text(size = 15))
```

---
# Summary
Data preparation:
 - No dedup/imputation performed
 - Construct new feature to avoid multicolinearity between yrs.since.phd and yrs.service
 - 8:2 train/test split, 5-fold CV inside the training set for hyperparameter tuning
 - Dummy code all factors; Add 2nd order polynomials on yrs.service; Normalize numeric features

Modeling:
 - Salary (logged): a general penalized regression achieving 0.08 RMSE on test set.
 - Binary salary indicator: a CART achieving 0.67 accuracy on test set.

Enhancement: 
 - Questions: causal relationship between gender and salary, interaction salary effect of gender on seniority, subgroup detection
 - Additional attributes: citations, PhD at top schools, current academic instition rank, working hours, marriage status
 - Sample: size >=45, oversampling high salary observations

---
# Analysis
(1) What percentage of records are Assistant Professors with less than 5 years of experience?
```{r}
#print an answer to the question 1
cat((nrow(dta%>%filter(rank=="AsstProf",yrs.service<5))*100/nrow(dta))%>%round(2),
    "% of records are Assistant Professors with less than 5 years of experience.",
    sep = "")
```

(2) Is there a statistically significant difference between female and male salaries?

 - Here we want to test the salary against two sex categories. Salary data does not follow a normal distribution in our sample, so we adopt the 2-sample Wilcoxon test. 

.pull-left[
```{r}
#view gender difference in salary
dta%>%group_by(sex)%>%summarise(mean(salary))
```
]

.pull-right[
```{r}
#perform wilcoxon test
wilcox.test(salary~sex,data = dta)
```
]

 - The p-value is smaller than 0.01, strongly against the null hypothesis that male and female holds equal salary. In other words, female and male salaries are significantly different. 

---
# Analysis
(3) What is the distribution of salary by rank and discipline?
```{r,fig.height=3.5}
#ggplot2 framwork
rank_plot <- ggplot(dta,
                    aes(x = rank %>% reorder(dta$salary),
                        y = salary)) +
  geom_boxplot(fill = "skyblue2")
discipline_plot <- ggplot(dta,
                          aes(x = discipline %>% reorder(dta$salary),
                              y = salary)) +
  geom_boxplot(fill = "skyblue2")
#a function to make the plots more pretty
plot_pretty <- function(plot) {
  pretty_plot <- plot + 
    xlab("") +
    ylab("") +
    theme_classic() +
    theme(text = element_text(size = 15)) 
  return(pretty_plot)
}
#integrate the two plots
grid.arrange(plot_pretty(rank_plot),
             plot_pretty(discipline_plot),
             nrow = 1)
```
 - We can see higher tank and discipline B (compared with A) is associated with higher salary.

(4) How would you recode discipline as a 0/1 binary indicator?

```{r, echo=TRUE}
discipline_B<-(dta$discipline=="B")%>%as.numeric()#dummy code it to avoid redundancy/collinearity. will use dplyr::mutate if manipulating a dataframe
```

```{r}
table(discipline_B)#view the result
```

---
exclude: true
```{r}
#inspect missing values
missing_glimpse(dta)
```
---
#Modeling I: Data preparation

 - Assume no duplicates due to no unique id column
 - No need for missing value imputations as the data is complete
 - Years since phd and years of service is strongly correlated.
```{r}
#test correlation between years of service and years since phd
cor.test(dta$yrs.since.phd, dta$yrs.service, method="pearson")
```

```{r, echo=TRUE}
dta<-dta%>%mutate(gap=yrs.since.phd-yrs.service)#manually construct a variable measuring gap between year of phd graduation and year starting service
```

```{r}
#view summary
dta<-dta%>%select(-yrs.since.phd)
summary(dta$gap)
```

---
#Modeling I: Preprocessing

 - Use 80% records to train data since we don't have a big sample
 - Apply 5-fold CV inside the training set for hyperparameter tuning (to make things faster)
 - The preprocessing steps (recipe object in tidymodels framework):

```{r}
set.seed(1)
#initial 8:2 split betwee train test set
dta_split <- initial_split(dta, prop = .8)
train <- training(dta_split)
test  <- testing(dta_split)
#define 5 fold cv in training set to tune hyperparameters
vfold <- vfold_cv(train, v = 5)
```

```{r}
#modeling framework
formula.reg <- as.formula(salary ~ .)
salary_rec <- recipe(formula.reg,
                     data = train) %>%
  step_log(all_outcomes(), base = 10) %>%
  step_string2factor(all_nominal(),-all_outcomes()) %>%
  step_dummy(all_nominal(),-all_outcomes()) %>%
  step_poly(yrs.service, degree = 2)  %>%
  step_normalize(all_predictors(),-all_nominal())

salary_rec
```

---
#Modeling I: Modeling choice

 - Start with a general penalized regression as baseline. 
 - It's quick, interpratable, and can usually serve as variable selection for subsequent modeling. 
 - The model definition:
 
```{r}
#define glmn model
glmn_model <-
  linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet") %>%
  set_mode("regression")
glmn_model
```

---
#Modeling I: Tuning the best model

 - We use a random grid searching 10 levels for each of mixture and penalty
 - We use RMSE as the metric to evaluate performance to give a relatively high weight to large errors since it's usually difficulty to predict them in salary
 
```{r,results=FALSE}
#define parameter grid
glmn_grid <- glmn_model %>% parameters() %>% grid_regular(levels = c(10,10))
#grid search
glmn_reg_search <-
  tune_grid(salary_rec,
            model = glmn_model,
            resamples = vfold,
            grid = glmn_grid)
show_best(glmn_reg_search, metric = "rmse", maximize = FALSE)
```
 
 
```{r,fig.height=3.5}
#visualize CVed performance on parameter grids
rmse_vals <- collect_metrics(glmn_reg_search) %>% filter(.metric == "rmse")
rmse_vals %>% 
  mutate(mixture = format(mixture)) %>% 
  ggplot(aes(x = penalty, y = mean, col = mixture)) + 
  ylab("RMSE") +
  geom_line() + 
  scale_x_log10() + 
  scale_color_brewer() +
  theme_classic() +
  theme(text = element_text(size = 15)) 
```

 - The best model:

```{r}
#inspect the best coefficient
best_glmn <-
  select_best(glmn_reg_search, metric = "rmse", maximize = FALSE)
best_glmn
```
 - The penalty and mixture are relatively small, showing that most variables have good prediction power

---
#Modeling I: Variable importance
```{r}
#finalize the recipe
final_rec <- salary_rec %>% prep()
#finalize the best trained glmn model
final_glmn_model <-
  glmn_model %>%
  finalize_model(best_glmn) 
fit_final_glmn_model <- final_glmn_model%>%
  fit(formula.reg, data = juice(final_rec))
```

```{r}
#plot var importance
vip(fit_final_glmn_model,
    fill = "skyblue2")+
  xlab ("Importance (coefficient magnitude)") +
  theme_classic() +
  theme(text = element_text(size = 15))
```

 - Being a professor and in discipline B have good chance to predict higher salary
 - Being an assistant professorl lead to lower salary
 - Quadratic term of years of service has negative coefficient. Multicolinearity? 

---
#Modeling I: Residual analysis on test set
```{r, fig.height=3.5}
#preprocess the test set using the same recipe
test_baked<-bake(final_rec,test)
#plot residual
test_prediction<-cbind(.pred=predict(fit_final_glmn_model,new_data = test_baked),
                       salary=test_baked$salary)
ggplot(test_prediction, aes(x = .pred, y = salary)) + 
  geom_abline(col = "skyblue2") + 
  geom_point(alpha = .8) + 
  xlab("Predicted value") +
  ylab("Salary") +
  theme_classic() +
  theme(text = element_text(size = 15))
```

```{r}
#calculate RMSE on test set
test_prediction%>%rmse(truth=salary,
                       estimate=.pred)
```

 - It's harder to predict higher salary well. 
 - In a larger workflow, I will use this as a baseline and try a few more models (e.g. tree algorithms which are less sensitive to extreme values).
 - Since this is a showcase example, I'll stop here. 

---
#Modeling II: Preprocessing and modeling choice

```{r, echo=TRUE}
dta <- dta %>% mutate(salary = (salary >= median(dta$salary)) %>% as.numeric()%>% as.factor())#Dummy code the salary variable
```

```{r}
#inspect salary indicator
table(dta$salary)
```

 - No need for resampling since the response variable is quite balanced
 
```{r}
set.seed(1)
#initial 8:2 split betwee train test set
dta_split <- initial_split(dta, prop = .8)
train <- training(dta_split)
test  <- testing(dta_split)
#define 5 fold cv in training set to tune hyperparameters
vfold <- vfold_cv(train, v = 5)
```

 - Same train/test split and preprocessing steps

```{r}
#modeling framework
formula.reg <- as.formula(salary ~ .)
salary_rec <- recipe(formula.reg,
                     data = train) %>%
  step_string2factor(all_nominal(),-all_outcomes()) %>%
  step_dummy(all_nominal(),-all_outcomes()) %>%
  step_poly(yrs.service, degree = 2)  %>%
  step_normalize(all_predictors(),-all_nominal())
```

 - For classification I usually start with logistic regressions as baseline
 - Since we used glmnet already, let's do a CART instead.
 
```{r}
#define glmn model
cart_model <-
  decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>%
  set_mode("classification")
cart_model 
```

---
#Modeling II: Tuning the best model

 - We use a random grid search for cost complexity and minimum points in a node with 10 levels for each (to save time).
 - Use accuracy as the evaluation metric since we don't have a particular weight among 0/1 label
 
```{r,results=FALSE}
#define parameter grid
cart_grid <- cart_model %>% parameters() %>% grid_regular(levels = c(5,5))
#grid search
ctrl <- control_grid(save_pred = TRUE)
cart_reg_search <-
  tune_grid(salary_rec,
            model = cart_model,
            resamples = vfold,
            grid = cart_grid,
            control = ctrl)
show_best(cart_reg_search, metric = "accuracy", maximize = FALSE)
```
 
 - The best model:
```{r}
best_cart <-
  select_best(cart_reg_search, metric = "accuracy", maximize = FALSE)
#inspect the best coefficient
best_cart
```

 - The n_min is small since our sample size is not big. 

---
#Modeling II: Performance on test set

```{r}
#finalize the recipe
final_rec <- salary_rec %>% prep()
#finalize the best trained glmn model
final_cart_model <-
  cart_model %>%
  finalize_model(best_cart) 
fit_final_cart_model <- final_cart_model%>%
  fit(formula.reg, data = juice(final_rec))
```

```{r, fig.height=4}
#preprocess the test set using the same recipe
test_baked <- bake(final_rec, test)
#plot residual
test_prediction <-
  cbind(
    .pred_class = predict(fit_final_cart_model, new_data = test_baked),
    salary = test_baked$salary
  ) %>% mutate(.pred_class = .pred_class %>% as.factor())
conf_mat_test<-conf_mat(test_prediction,
         truth = salary,
         estimate = .pred_class)
autoplot(conf_mat_test, type = "heatmap")+
  theme_classic() +
  theme(text = element_text(size = 15))
```

```{r}
#calculate accuracy on test set
test_prediction %>% accuracy(truth = salary,
                             estimate = .pred_class)
```

 - The performance has quite a balance between FN and FP.
 - However, CART model did worse on the test set which means it overfits. Next step would be to use bagged trees such as RF or GBM to reduce overfitting. 
 
---
#Data Set Enhancement: 3 further questions
1. Is there a causal relationship between gender and salaries among PhDs?

 - From the analysis part, we know that there's a significant salary gap among male and female. However, are female discriminated in the labor market due to their gender identity? In other words, is the gap induced by the gender identify itself rather than other difference (e.g. rank, discipline or seniority)? We can use a linear regression to observe the significance of the gender coefficient controlling other parameters. 

2. Will gender identity influence the reward to seniority?

 - The mechanism for gender identity to impact labor market rewards is complicated. In addition to pure discrimination described by the previous case, it's also possible that women with seniority will receive less reward than men with seniority, due to an invisible salary ceiling or inability to work very hard (e.g. family obligations so cannot publish a lot). In other words, there might be an interaction effect between sex and ys.service. 
 
3. What are the subgroups in the academic population? 

 - Rather than the male vs. female or B vs. A difference, can we adopt a more holistic way to observe subgroups in the PhDs, who share similarity in their demographics and academic profile? We can achieve this by applying a clustering algorithm to the data (e.g. K-means or K-medoid since the sample size is small and data is mixed). 
 
---
#Data Set Enhancement: 5 additional attributes
As the previous questions targeted salary, it's better to have measurements of other the  eterminants of salary to avoid omitted variable bias.
1. Number of publications/citations (e.g. 34 citations for SCI)
 - More publications usually lead to faster promotion and higher salary. If we add it we can determine whether the gender salary gap can rise from the publication difference. Data might be obtained with tools such as Elsevier API if real names are given. 
 
2. PhD at a top school (e.g. Yes)
 - A degree from a top tier school such as Harvard always brings a premium, regardless of the curent place that someone teaches in, as another dimension of the academic profile. 
 
3. Current academic institution rank (e.g. 102th on U.S. News)
 - Teaching in a top tier university versus a community college brings huge difference in career prospect, and we want to put in as a confounding variable to avoid introducing bias in other estimates. 

---
#Data Set Enhancement: 5 additional attributes

4. Weekly working hours (e.g. 30 hours)
 - This is a potential proxy for the mechanism of gender gap. If female PhDs are only able to work fewer hours than male, then the significance of the coefficient for gender will likely dissapear after adding working hours. 
 
5. Marriage status (e.g. Unmarried)
 - For a similar reason, marriage might be a proxy for family obligations, since female usually have to spend more time at home after getting married. Adding that will be helpful to clarify the mechanism of gender influence.

---
#Data Set Enhancement: Sample size

We use a power analysis to calculate a minimum requirement for sample size. 

```{r,echo=TRUE}
library(pwr)
pwr.f2.test(
  u = 10,#10 independent variables (less intercept)
  f2 = 0.5/(1-0.5),#Assume the R2 is 0.5
  sig.level = 0.001,#Need 0.001 significance level 
  power = 0.8#Assuming 0.8 power
)
```

Therefore, the minimum sample size is v+u+1=45. We are well beyond this threhold.

---
#Data Set Enhancement: Sampling design
However, it's also important to obtain a good estimate of population from the sample, which requires a representative sampling design. 

```{r,fig.height=3.5}
salary_plot#pull up y plot again
```

Since most records are in the low to medium tier of the income, it would be the best if we can oversample the high salary group (e.g.>150,000) with techniques such as stratified sampling. Having more records in that group will help to form better estimates.

The case study ends here. Thank you very much for taking the time to read! 
 